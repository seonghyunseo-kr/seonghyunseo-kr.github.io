---
layout: post
title: "Paper Review: N-BEATS (Neural basis expansion analysis)"
date: 2026-01-15
tags: [time-series, paper-review, forecasting]
style: border
color: primary
comments: false
description: "A quick review of N-BEATS: an interpretable deep learning model for univariate time-series forecasting."
toc: true
---

## TL;DR
- N-BEATS is a pure feed-forward architecture that repeatedly models residuals (backcast/forecast).
- Strong performance on univariate forecasting without specialized time-series components.
- Interpretable variant uses basis expansions (trend/seasonality).

## Paper / Resources
- Paper: https://arxiv.org/abs/1905.10437
- Code: (add link)

## Key Idea
A stack of blocks predicts:
- **Backcast**: what portion of the past the block explains
- **Forecast**: the future prediction contribution

Residual learning:
- Update residual input by subtracting backcast
- Sum forecasts across blocks for final prediction

## What I found important
- Works surprisingly well for a “simple” MLP-based model
- Structured variant gives interpretability via basis functions
- Good baseline for univariate forecasting

## My Takeaways
- Always compare against N-BEATS when evaluating new univariate forecasting models
- Residual stacking is a powerful design pattern

## Questions
- How does it compare under covariates / multivariate settings?
- When do transformers beat it for forecasting?

## References
- Related deep forecasting models (DeepAR, TFT, etc.)