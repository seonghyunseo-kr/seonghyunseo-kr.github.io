---
layout: post
title: "Paper Review: An Image is Worth 16x16 Words (ViT)"
date: 2026-01-16
tags: [computer-vision, paper-review, transformer]
style: border
color: primary
comments: false
description: "A quick review of Vision Transformer (ViT): core idea, architecture, and practical takeaways."
toc: true
---

## Overview
ViT reframes image classification as a **sequence modeling** problem: split an image into fixed-size patches, embed them as tokens, and apply a standard Transformer encoder.

## Paper / Resources
- Paper: https://arxiv.org/abs/2010.11929
- Code (official): https://github.com/google-research/vision_transformer

## Key Idea
1. **Patchify** an image into $$N$$ patches of size $$P \times P$$  
2. **Linear projection**: each patch → token embedding  
3. Add **positional embeddings** (+ optional `[CLS]` token)  
4. Feed into a **Transformer encoder**  
5. Use `[CLS]` (or pooling) for classification

## Why it works (in practice)
- With **large-scale pretraining**, ViT can match or outperform strong CNN baselines.
- Attention can capture **global interactions** between patches early (vs. local receptive fields in CNNs).

## Notes I want to remember
- Data scale matters a lot; without it, CNN inductive biases can still be beneficial.
- Patch size is a compute/accuracy trade-off:
  - smaller patches → longer sequence → more compute
  - larger patches → less detail, cheaper

## Questions
- What’s the most effective way to make ViTs **data-efficient**? (distillation, augmentation, hybrid models)
- What are good defaults for patch size and model size under limited compute?

## (Optional) Experiments
- Setup:
- Results:
- Takeaways: